#! /usr/bin/env bash

RED="\e[31m"
GREEN="\e[32m"
EC="\e[0m"

KUBECONFIG="${kubeconfig_path}"
KUBECONFIG_PROXY="$KUBECONFIG-proxy"
TUNNEL_SOCKET_FILE=$${TUNNEL_SOCKET_FILE:-/tmp/k8s-tunnel-socket-${k8s_tunnel_port}}
kubeconfig=""

open_ssh_tunnel_to_k8s_api() {
  if [[ -n "${bastion_public_ip}" && -n "${bastion_user}" ]]; then
    close_ssh_tunnel_to_k8s_api >/dev/null 2>&1 || true
    printf "$GREEN Opening k8s tunnel ... $EC \n"
    ssh-keygen -R "${bastion_public_ip}" >/dev/null 2>&1 || true

    chmod 400 "${ssh_pvt_key_path}" || true
    if ssh -v -q -N -f -M -o "IdentitiesOnly=yes" -o "StrictHostKeyChecking=no" -o "ExitOnForwardFailure=yes" -i "${ssh_pvt_key_path}" -D ${k8s_tunnel_port} -S "$TUNNEL_SOCKET_FILE" ${bastion_user}@${bastion_public_ip}; then
      printf "$GREEN Established k8s ssh tunnel. $EC\n"
    else
      printf "$RED Failed to establish k8s ssh tunnel. $EC\n"
      return 1
    fi
  else
    printf "$GREEN No bastion, no tunnel needed... $EC\n"
  fi
}

check_kubeconfig() {
  printf "$GREEN Checking if $KUBECONFIG exists... $EC\n"
  if test -f "$KUBECONFIG"; then
    if [[ -n "${bastion_public_ip}" ]]; then
      echo "$KUBECONFIG exists, creating $KUBECONFIG_PROXY for proxy use."
      cp $KUBECONFIG $KUBECONFIG_PROXY
      kubectl --kubeconfig $KUBECONFIG_PROXY config set "clusters.${eks_cluster_arn}.proxy-url" "socks5://127.0.0.1:${k8s_tunnel_port}"
      kubeconfig=$KUBECONFIG_PROXY
    else
      kubeconfig=$KUBECONFIG
    fi
  else
    echo "$KUBECONFIG does not exist." && exit 1
  fi
  export KUBECONFIG="$kubeconfig"
  echo
}

set_k8s_auth() {
  local AWS_AUTH_YAML="${aws_auth_yaml}"
  if test -f "$AWS_AUTH_YAML"; then
    printf "$GREEN Updating $AWS_AUTH_YAML... $EC \n"
    kubectl_apply "$AWS_AUTH_YAML"
  else
    printf "$RED $AWS_AUTH_YAML does not exist. $EC \n" && exit 1
  fi
  echo
}

remove_calico_cr() {
  local WAIT_TIME=300
  local SLEEP_TIME=5
  local COUNTER=0

  printf "$GREEN Removing Calico CRDs...$EC \n"
  kubectl_cmd delete installation default --ignore-not-found=true

  while kubectl --kubeconfig "$kubeconfig" get namespaces -o=jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | grep -q '^calico-system$'; do
    if [ $COUNTER -ge $WAIT_TIME ]; then
      printf "$RED Timed out waiting for calico-system namespace to be fully deleted.$EC \n"
      exit 1
    fi

    printf "$GREEN Waiting for calico-system namespace to be fully deleted...$EC \n"
    sleep $SLEEP_TIME
    COUNTER=$((COUNTER + SLEEP_TIME))
  done
  printf "$GREEN calico-system namespace was deleted successfully...$EC \n"
}

remove_tigera_operator() {
  printf "$GREEN Removing existing Tigera operator...$EC \n"
  kubectl_cmd delete -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/tigera-operator.yaml \
    --ignore-not-found=true \
    --force \
    --grace-period=0
  echo
  printf "$GREEN Tigera operator was removed successfully...$EC \n"
}

check_remove_calico() {
  local OPERATOR_DEPLOYMENT_NAME="tigera-operator"
  local OPERATOR_NAMESPACE="tigera-operator"
  local calico_deploy=""
  local managed_by=""

  calico_deploy="$(kubectl --kubeconfig "$kubeconfig" get deployment $OPERATOR_DEPLOYMENT_NAME -n $OPERATOR_NAMESPACE --no-headers=true --ignore-not-found=true)"

  if [ -n "$calico_deploy" ]; then
    managed_by=$(kubectl --kubeconfig "$kubeconfig" get deployment $OPERATOR_DEPLOYMENT_NAME -n $OPERATOR_NAMESPACE -o jsonpath="{.metadata.labels['app\.kubernetes\.io/managed-by']}")
    if [ -z "$managed_by" ] || [ "$managed_by" != "Helm" ]; then
      remove_calico_cr
      remove_tigera_operator
    fi
  fi
}

install_calico() {
  check_remove_calico

  local max_retries=3
  local sleep_duration=10

  for i in $(seq 1 $max_retries); do
    helm_cmd upgrade "calico-tigera-operator" \
      tigera-operator \
      --repo "https://projectcalico.docs.tigera.io/charts" \
      --version "${calico_version}" \
      --kubeconfig "$kubeconfig" \
      --namespace "tigera-operator" \
      --set installation.kubernetesProvider=EKS \
      --set installation.cni.type=AmazonVPC \
      --set installation.registry="quay.io/" \
      --timeout 10m \
      --create-namespace \
      --install

    if [ $? -eq 0 ]; then
      break
    fi

    if [ $i -lt $max_retries ]; then
      echo "Attempt $i failed. Retrying in $${sleep_duration}s..."
      sleep $sleep_duration
    else
      printf "$RED Maximum attempts reached. Exiting. $EC \n"
      exit 1
    fi

  done
}

validate_url() {
  local url="$1"
  local log_file="validate-url.log"
  if curl --head --fail --max-time 10 --output "$log_file" --stderr "$log_file" "$url"; then
    rm "$log_file" && return 0
  else
    cat "$log_file" && return 1
  fi
}

kubectl_apply() {
  local k8s_manifest=$1
  if test -f "$k8s_manifest" || validate_url "$k8s_manifest"; then
    printf "$GREEN Applying $k8s_manifest...$EC \n"
    kubectl_cmd apply -f $k8s_manifest
  else
    printf "$RED $k8s_manifest does not exist. $EC \n"
    exit 1
  fi
}

helm_cmd() {
  printf "Running helm $@...\n"
  helm --kubeconfig "$kubeconfig" $@
  local exit_code=$?
  if [ $exit_code -ne 0 ]; then
    printf "$RED Error running helm $@ $EC \n"
  fi
  return $exit_code
}

kubectl_cmd() {
  printf "kubectl $@...\n"
  kubectl --kubeconfig "$kubeconfig" $@
  if [ $? -ne 0 ]; then
    printf "$RED Error running kubectl $@ $EC \n"
    exit 1
  fi
}

wait_for_single_node() {
  TIMEOUT=600
  ELAPSED_TIME=0
  SLEEP_INTERVAL=30

  while true; do
    READY_NODES_COUNT=$(kubectl --kubeconfig "$kubeconfig" get nodes -l 'single-node' -o json | jq '[.items[] | .status.conditions[] | select(.type=="Ready" and .status=="True")] | length')

    if [[ "$READY_NODES_COUNT" -ge 1 ]]; then
      echo "At least one node with label 'single-node' is in Ready status!"
      kubectl --kubeconfig "$kubeconfig" get nodes -l 'single-node' -o wide
      return 0
    else
      echo "Waiting for node with label 'single-node' to be in Ready status..."
      sleep $SLEEP_INTERVAL
      ELAPSED_TIME=$((ELAPSED_TIME + SLEEP_INTERVAL))
      if [[ "$ELAPSED_TIME" -ge "$TIMEOUT" ]]; then
        echo "Timeout reached. Exiting."
        exit 1
      fi
    fi
  done
}

close_ssh_tunnel_to_k8s_api() {
  if [[ -n "${bastion_public_ip}" ]]; then
    printf "$GREEN Shutting down k8s tunnel ... $EC"
    ssh -S $TUNNEL_SOCKET_FILE -O exit ${bastion_user}@${bastion_public_ip}
  fi
}
